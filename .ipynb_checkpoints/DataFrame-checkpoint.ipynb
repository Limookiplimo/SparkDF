{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d9313fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row, Column\n",
    "from pyspark.sql.functions import upper, pandas_udf\n",
    "import pandas as pd\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0646d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 14:48:00 WARN Utils: Your hostname, drice resolves to a loopback address: 127.0.1.1; using 192.168.37.109 instead (on interface wlp0s20f3)\n",
      "23/04/28 14:48:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/28 14:48:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fed7090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of Rows\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1,b=2.,c='string1',d=date(2001,1,1),e=datetime(2001,1,1,12,0)),\n",
    "    Row(a=2,b=3.,c='string2',d=date(2001,2,1),e=datetime(2001,2,1,12,0)),\n",
    "    Row(a=3,b=4.,c='string3',d=date(2001,3,1),e=datetime(2001,3,1,12,0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15b339e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2001-01-01|2001-01-01 12:00:00|\n",
      "|  2|3.0|string2|2001-02-01|2001-02-01 12:00:00|\n",
      "|  3|4.0|string3|2001-03-01|2001-03-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d05a77fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explicit schema\n",
    "df = spark.createDataFrame([\n",
    "    (1,2.,'string1', date(2000,1,1),datetime(2000,1,1,0)),\n",
    "    (2,3.,'string2', date(2000,1,1),datetime(2000,1,1,0)),\n",
    "    (3,4.,'string3', date(2000,1,1),datetime(2000,1,1,0))\n",
    "],\n",
    "schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1cd6820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 00:00:00|\n",
      "|  2|3.0|string2|2000-01-01|2000-01-01 00:00:00|\n",
      "|  3|4.0|string3|2000-01-01|2000-01-01 00:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ffe2107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From Pandas\n",
    "pd_df = pd.DataFrame({\n",
    "    'a':[1,2,3],\n",
    "    'b':[2.,3.,4.],\n",
    "    'c':['string1','string2','string3'],\n",
    "    'd':[date(2000,1,1),date(2000,2,1),date(2000,3,1)],\n",
    "    'e':[datetime(2000,1,1,12,0),datetime(2000,2,1,12,0),datetime(2000,3,1,12,0)]\n",
    "})\n",
    "df = spark.createDataFrame(pd_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1cb8f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-02-01 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-03-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d640867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+-------+\n",
      "|summary|  a|  b|      c|\n",
      "+-------+---+---+-------+\n",
      "|  count|  3|  3|      3|\n",
      "|   mean|2.0|3.0|   null|\n",
      "| stddev|1.0|1.0|   null|\n",
      "|    min|  1|2.0|string1|\n",
      "|    max|  3|4.0|string3|\n",
      "+-------+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sumary\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a86f9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 2, 1, 12, 0)),\n",
       " Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 3, 1, 12, 0))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Collect\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1d78c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7ed8092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 3, 1, 12, 0))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tail\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "676bda43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limoo/anaconda3/envs/spark/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-02-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-03-01 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b        c           d                   e\n",
       "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
       "1  2  3.0  string2  2000-02-01 2000-02-01 12:00:00\n",
       "2  3  4.0  string3  2000-03-01 2000-03-01 12:00:00"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Pandas\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85a2c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|UPPER_C|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n",
      "|  2|3.0|string2|2000-02-01|2000-02-01 12:00:00|STRING2|\n",
      "|  3|4.0|string3|2000-03-01|2000-03-01 12:00:00|STRING3|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Column Operations\n",
    "df.withColumn('UPPER_C',upper(df.c)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fd012d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  3|4.0|string3|2000-03-01|2000-03-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter\n",
    "df.filter(df.b =='4.0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c663dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|pan_series(b)|\n",
      "+-------------+\n",
      "|            2|\n",
      "|            3|\n",
      "|            4|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pandas UDF\n",
    "@pandas_udf('long')\n",
    "def pan_series(series: pd.Series) -> pd.Series:\n",
    "    return series\n",
    "df.select(pan_series(df.b)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bae628d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  2|3.0|string2|2000-02-01|2000-02-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Map in Pandas\n",
    "def pan_filter(iterator):\n",
    "    for pan_df in iterator:\n",
    "        yield pan_df[pan_df.a==2]\n",
    "df.mapInPandas(pan_filter, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "168f7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame([\n",
    "    ['red','harrier',1,1000],\n",
    "    ['black','nissan',1,900],\n",
    "    ['red','probox',5,250],\n",
    "    ['grey', 'axio',2,700],\n",
    "    ['blue','fit',3,350]\n",
    "],\n",
    "schema = ['color', 'type', 'quantity','price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e31a8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|color|avg(price)|\n",
      "+-----+----------+\n",
      "|  red|     625.0|\n",
      "|black|     900.0|\n",
      "| grey|     700.0|\n",
      "| blue|     350.0|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group\n",
    "n=new_df.groupby('color').avg('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9f1fc5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   type|avg(price)|\n",
      "+-------+----------+\n",
      "|harrier|    1000.0|\n",
      "| nissan|     900.0|\n",
      "| probox|     250.0|\n",
      "|   axio|     700.0|\n",
      "|    fit|     350.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mean\n",
    "new_df.groupby('type').mean('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6fe2dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------+-----+\n",
      "|color|   type|quantity|price|\n",
      "+-----+-------+--------+-----+\n",
      "|black| nissan|       1| 1800|\n",
      "| blue|    fit|       3|  700|\n",
      "| grey|   axio|       2| 1400|\n",
      "|  red|harrier|       1| 1625|\n",
      "|  red| probox|       5|  875|\n",
      "+-----+-------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Apply Python Function to Groups\n",
    "def add_mean(pan_df):\n",
    "    return pan_df.assign(price=pan_df.price + pan_df.price.mean())\n",
    "new_df.groupby('color').applyInPandas(add_mean, schema=new_df.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "faaee299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------+-----+\n",
      "|  inv| amount|    time|deliv|\n",
      "+-----+-------+--------+-----+\n",
      "|50020|10000.0|20000101|    n|\n",
      "|50021| 2055.0|20000102|    y|\n",
      "|50022| 1000.0|20000103| null|\n",
      "|50023|20600.0|20000104| null|\n",
      "|50024|25500.0|20000105| null|\n",
      "|50025|20500.0|20000105|    n|\n",
      "+-----+-------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Co-Group\n",
    "df1=spark.createDataFrame([\n",
    "    (50020,10000,20000101),\n",
    "    (50021,2055,20000102),\n",
    "    (50022,1000,20000103),\n",
    "    (50023,20600,20000104),\n",
    "    (50024,25500,20000105),\n",
    "    (50025,20500,20000105)\n",
    "],('inv','amount','time'))\n",
    "\n",
    "df2=spark.createDataFrame([\n",
    "    (50021,2055,'y'),\n",
    "    (50020,10000,'n'),\n",
    "    (50025,20500,'n'),\n",
    "    (50023,20600,'y')\n",
    "],('inv','amount','deliv'))\n",
    "\n",
    "def inv_merge(l, r):\n",
    "    return pd.merge_ordered(l, r)\n",
    "\n",
    "df1.groupby('inv').cogroup(df2.groupby('inv'))\\\n",
    ".applyInPandas(inv_merge, schema='inv int,amount float,time int, deliv string').show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e195f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
